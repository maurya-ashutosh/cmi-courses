{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dd5d3821",
      "metadata": {
        "id": "dd5d3821"
      },
      "source": [
        "# Natural Language Processing\n",
        "# Assignment 1\n",
        "Ashutosh Maurya (MDS202110)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02b36a9c",
      "metadata": {
        "id": "02b36a9c"
      },
      "source": [
        "## Corpus Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72b640d6",
      "metadata": {
        "id": "72b640d6"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import nltk\n",
        "import copy\n",
        "import regex as re\n",
        "import string\n",
        "import pickle\n",
        "\n",
        "from collections import Counter\n",
        "from collections import OrderedDict\n",
        "from itertools import tee, islice\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a3120ee",
      "metadata": {
        "id": "8a3120ee"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "zip_folder = zipfile.ZipFile('pdf_json.zip', 'r')\n",
        "# imgdata = archive.read('img_01.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "219870f0",
      "metadata": {
        "id": "219870f0"
      },
      "outputs": [],
      "source": [
        "zip_path = 'pdf_json.zip'\n",
        "zip_folder = zipfile.ZipFile(zip_path, 'r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49fd22f6",
      "metadata": {
        "scrolled": true,
        "id": "49fd22f6",
        "outputId": "17a420d4-e8cd-4aeb-fc49-04306e00f47f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "56529"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(zip_folder.namelist())-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbc834e7",
      "metadata": {
        "id": "fbc834e7"
      },
      "outputs": [],
      "source": [
        "### extract_body_text(file):\n",
        "### input: .json file\n",
        "### output: stores the content of the 'body_text' tag of the input file into a string\n",
        "### also transforms all text into lowercase\n",
        "\n",
        "\n",
        "def extract_body_text(file):\n",
        "\n",
        "    paper_content = json.load(file)\n",
        "    body_text = \"\"\n",
        "    if 'body_text' in paper_content:\n",
        "#         print(\"----DOC STARTS----\")\n",
        "        for bt in paper_content ['body_text']:\n",
        "            body_text = (body_text + bt ['text']).lower()\n",
        "\n",
        "#             print((body_text + '\\n').lower ())\n",
        "\n",
        "#         print(\"----DOC ENDS----\")\n",
        "    return body_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4264162f",
      "metadata": {
        "id": "4264162f"
      },
      "outputs": [],
      "source": [
        "### make_corpus(n_docs):\n",
        "### input: number of files(documents)\n",
        "### output: stores the output from extract_body_text(file) for the given number of files\n",
        "### check for .json as some files weren't json\n",
        "\n",
        "\n",
        "def make_corpus(n_docs=56529):\n",
        "    count = 0\n",
        "    corpus = \"\"\n",
        "#     n_docs = 10\n",
        "    with zipfile.ZipFile('pdf_json.zip') as z:\n",
        "        for file in z.namelist():\n",
        "            if not os.path.isdir(file) and file.filename.endswith('.json'):\n",
        "                # read the file\n",
        "                with z.open(file) as f:\n",
        "                    corpus += extract_body_text(f)\n",
        "                count += 1\n",
        "            if count>=n_docs:\n",
        "                break\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6807b7ec",
      "metadata": {
        "id": "6807b7ec",
        "outputId": "368fe75e-6774-4931-e562-584c4fc11daa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 12min 20s\n",
            "Wall time: 12min 21s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "corpus = make_corpus(50000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e96f9bd",
      "metadata": {
        "id": "2e96f9bd"
      },
      "outputs": [],
      "source": [
        "pickle.dump(corpus,open('corpus.txt','w'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2089ea4",
      "metadata": {
        "id": "b2089ea4"
      },
      "outputs": [],
      "source": [
        "with open('corpus.txt', \"r\", encoding = \"utf8\") as file:\n",
        "    full_corpus = file.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb304f11",
      "metadata": {
        "id": "bb304f11"
      },
      "outputs": [],
      "source": [
        "full_corpus = str(full_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "426bca66",
      "metadata": {
        "scrolled": true,
        "id": "426bca66",
        "outputId": "fe121c3a-027a-4d41-a3f9-3f52414925e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1264601399"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(full_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67e64621",
      "metadata": {
        "id": "67e64621"
      },
      "source": [
        "The variable ```full_corpus``` contains the required corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "964459cb",
      "metadata": {
        "id": "964459cb"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "- We shall first tokenize the sentences using ```nltk.tokenize.sent_tokenize```, because we need to tag the sentences with a start and end tag.\n",
        "- Since our task is to predict missing text, we cannot remove stop words. Similarly, we cannot lemmatize the corpus.\n",
        "- We shall remove all punctuation, and any extra spaces\n",
        "- We shall remove all URLs.\n",
        "- We shall remove all digits.\n",
        "- We shall remove all special characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a608d0b",
      "metadata": {
        "id": "8a608d0b"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5061da81",
      "metadata": {
        "id": "5061da81"
      },
      "outputs": [],
      "source": [
        "### padding(n):\n",
        "### input: n for n-gram\n",
        "### output: corpus with begin and end sentence tags\n",
        "\n",
        "\n",
        "def padding(n):\n",
        "    token_corpus = []\n",
        "    for doc in full_corpus:\n",
        "        # adding 1 and 2 of either tags for bigrams and trigrams respectively\n",
        "        token_doc = [\"bgnsntnc \"*(n-1) + j.lower() + \" endsntnc\"*(n-1) for j in sent_tokenize(doc)]\n",
        "        token_corpus.append(token_doc)\n",
        "    return token_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3445ccb9",
      "metadata": {
        "id": "3445ccb9"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "corpus_bi = padding(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "671af8f4",
      "metadata": {
        "id": "671af8f4",
        "outputId": "3007f949-b7f6-4c78-ad21-a0f538534c56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 7min 9s\n",
            "Wall time: 7min 9s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "corpus_tri = padding(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4776b56",
      "metadata": {
        "id": "a4776b56"
      },
      "outputs": [],
      "source": [
        "# pickle.dump(corpus_bi,open('corpus_bi','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b66d8656",
      "metadata": {
        "id": "b66d8656"
      },
      "outputs": [],
      "source": [
        "# pickle.dump(corpus_tri,open('corpus_tri','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c98c1ce7",
      "metadata": {
        "id": "c98c1ce7"
      },
      "outputs": [],
      "source": [
        "corpus_bi = pickle.load(open('corpus_bi','rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b552d7fe",
      "metadata": {
        "id": "b552d7fe"
      },
      "outputs": [],
      "source": [
        "corpus_tri = pickle.load(open('corpus_tri','rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca8c8b92",
      "metadata": {
        "id": "ca8c8b92"
      },
      "outputs": [],
      "source": [
        "### preprocessing(corpus):\n",
        "### input: text corpus (list of lists of sentences)\n",
        "### output: preprocessed corpus (str)\n",
        "### applies the preprocessing steps using regex and string manupulation\n",
        "\n",
        "def preprocessing(corpus):\n",
        "\n",
        "    prep_corpus = copy.deepcopy(corpus)\n",
        "\n",
        "    prep_corpus = ' '.join(str(y) for x in prep_corpus for y in x)\n",
        "#     ref_pattern = re.compile(r'\\[[0-9/]+\\]')\n",
        "    url_pattern = re.compile(r'https?:\\/\\/.\\S*')\n",
        "    prep_corpus = re.sub(url_pattern,\" \",prep_corpus)\n",
        "    prep_corpus = \"\".join([char for char in prep_corpus if char.isalpha() or char.isspace()])\n",
        "    prep_corpus = re.sub('\\s+', ' ', prep_corpus).strip()\n",
        "\n",
        "    return prep_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8066ebe3",
      "metadata": {
        "id": "8066ebe3",
        "outputId": "7c1dcb9e-c8f4-4429-da63-17f932d7e3e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 6min 52s\n",
            "Wall time: 8min 49s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prep_corpus_bi = preprocessing(corpus_bi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a762614",
      "metadata": {
        "id": "3a762614",
        "outputId": "56c28c6f-6b5e-4ced-9735-18414eb2db37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 7min 49s\n",
            "Wall time: 20min\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prep_corpus_tri = preprocessing(corpus_tri)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86ec4cdb",
      "metadata": {
        "id": "86ec4cdb"
      },
      "source": [
        "The variables ```prep_corpus_bi``` and ```prep_corpus_bi``` contain the bigram and trigram preprocessed corpus respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9c55388",
      "metadata": {
        "id": "c9c55388"
      },
      "outputs": [],
      "source": [
        "# pickle.dump(prep_corpus_bi,open('prep_corpus_bi','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ff9f97f",
      "metadata": {
        "id": "6ff9f97f"
      },
      "outputs": [],
      "source": [
        "# pickle.dump(prep_corpus_tri,open('prep_corpus_tri','wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b507182",
      "metadata": {
        "id": "6b507182"
      },
      "source": [
        "## Vocabulary Count\n",
        "\n",
        "Counting the number of unique words in ```prep_corpus_bi``` will give us the vocabulary count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41fd310b",
      "metadata": {
        "scrolled": true,
        "id": "41fd310b",
        "outputId": "97fa8d63-9ae0-4967-f753-67cbe9f2d250"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 1.39 s\n",
            "Wall time: 3.81 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prep_corpus_bi = pickle.load(open('prep_corpus_bi','rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0435879f",
      "metadata": {
        "id": "0435879f",
        "outputId": "8040f2c1-ddd7-4fba-9d7d-93540c3eb3d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 1.5 s\n",
            "Wall time: 4.15 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prep_corpus_tri = pickle.load(open('prep_corpus_tri','rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d8261b5",
      "metadata": {
        "id": "6d8261b5",
        "outputId": "a358440b-d8b3-4b62-ae39-9607c7180266"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 23.5 s\n",
            "Wall time: 24.2 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "all_words_bi = prep_corpus_bi.split(\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a813ed41",
      "metadata": {
        "id": "a813ed41",
        "outputId": "bb970f75-e2a3-44d1-8534-0461e648269d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 4min 25s\n",
            "Wall time: 14min 8s\n"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "# pickle.dump(all_words_bi,open('all_words_bi','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02417c2a",
      "metadata": {
        "id": "02417c2a",
        "outputId": "2c8cff7f-5355-4168-d53f-20d55f3ce580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 35.3 s\n",
            "Wall time: 50.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "all_words_bi = pickle.load(open('all_words_bi','rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ca19cc2",
      "metadata": {
        "id": "1ca19cc2",
        "outputId": "8a89528b-9402-4a21-ea84-cf2e41cb7100"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 39.1 s\n",
            "Wall time: 47.6 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "all_words_tri = prep_corpus_tri.split(\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7424eeb3",
      "metadata": {
        "id": "7424eeb3",
        "outputId": "3240421f-325a-44b6-aa4c-1673d577315f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 42.6 s\n",
            "Wall time: 53.1 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "vocabulary = Counter(all_words_bi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0b4014e",
      "metadata": {
        "id": "b0b4014e"
      },
      "outputs": [],
      "source": [
        "# pickle.dump(vocabulary,open('vocabulary','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c85de0a",
      "metadata": {
        "id": "0c85de0a"
      },
      "outputs": [],
      "source": [
        "V = len(vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1a8000f",
      "metadata": {
        "id": "f1a8000f",
        "outputId": "7b694bfe-893e-4b99-8104-bbfcc0e96edc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1337035"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "V"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f59605c3",
      "metadata": {
        "id": "f59605c3"
      },
      "source": [
        "The vocabulary count is 1337035."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95ac9d38",
      "metadata": {
        "id": "95ac9d38"
      },
      "source": [
        "## Bigram and Trigram Language Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec73a678",
      "metadata": {
        "id": "ec73a678"
      },
      "outputs": [],
      "source": [
        "from itertools import tee, islice\n",
        "\n",
        "### ngrams(words, n):\n",
        "### input: words_list and n for n-gram\n",
        "### output: itertool object counting frequency\n",
        "### uses a generator object to iteratively yield the count\n",
        "\n",
        "def ngrams(word_list, n):\n",
        "    temp = word_list\n",
        "    while True:\n",
        "        first, second = tee(temp)\n",
        "        slice = tuple(islice(first, n))\n",
        "        if len(slice) == n:\n",
        "            yield slice\n",
        "            next(second)\n",
        "            temp = second\n",
        "        else:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b18dd65c",
      "metadata": {
        "id": "b18dd65c"
      },
      "outputs": [],
      "source": [
        "### make_model(words,n):\n",
        "### input: words_list and n for n-gram\n",
        "### output: frequency table (dictionary with tuples (ngrams) as keys and frequency as values)\n",
        "### using a Counter with the output of ngrams() allows us to count the frequency iteratively, which is faster\n",
        "\n",
        "def make_model(word_list,n_gram):\n",
        "\n",
        "    freq_table = Counter(ngrams(word_list, n_gram))\n",
        "#     freq_table = OrderedDict(freq_table.most_common())\n",
        "#     print(\"Most common {}-grams :\".format(n_gram), list(freq_table.items())[:10])\n",
        "\n",
        "    return freq_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a869fdd3",
      "metadata": {
        "id": "a869fdd3",
        "outputId": "a3785d82-05c7-426c-da1f-37ac2ee9d337"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 3.53 s\n",
            "Wall time: 6.19 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "unigram_model = OrderedDict(vocabulary.most_common())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "300ca642",
      "metadata": {
        "id": "300ca642",
        "outputId": "6cd2c832-a94a-471b-9638-b85ec22dbff5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 1.31 s\n",
            "Wall time: 2.4 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "unigram_model = pickle.load(open('unigram_model','rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e6d25eb",
      "metadata": {
        "id": "7e6d25eb"
      },
      "outputs": [],
      "source": [
        "# pickle.dump(unigram_model,open('unigram_model','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8df3894",
      "metadata": {
        "id": "f8df3894",
        "outputId": "ca814c30-3751-4a68-f7b0-77d02aa00aed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 4min 18s\n",
            "Wall time: 5min 34s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "bigram_model = make_model(all_words_bi, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbebec03",
      "metadata": {
        "id": "bbebec03",
        "outputId": "022cace4-faac-43dd-fe2a-5ab6e4813a1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 40.9 s\n",
            "Wall time: 1min 3s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "bigram_model = pickle.load(open('bigram_model','rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f292b77",
      "metadata": {
        "id": "5f292b77"
      },
      "outputs": [],
      "source": [
        "# pickle.dump(bigram_model,open('bigram_model','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3318c86c",
      "metadata": {
        "id": "3318c86c",
        "outputId": "b5b7fa7b-41fa-4b59-e7ac-3b52c9628a02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10403597"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unigram_model[\"the\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52fd77c9",
      "metadata": {
        "id": "52fd77c9",
        "outputId": "20ca0c46-89ce-4d11-82c1-247faed231de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "360116"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bigram_model[(\"and\",\"the\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2977722",
      "metadata": {
        "id": "e2977722",
        "outputId": "6a863fbd-e1f9-43e4-feaa-eb9b95611990"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 13min 40s\n",
            "Wall time: 48min 52s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "trigram_model = make_model(all_words_tri, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18f5b86c",
      "metadata": {
        "id": "18f5b86c",
        "outputId": "4f668001-dee7-457f-e1fa-ed021439f7d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "119"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trigram_model[(\"and\",\"the\",\"people\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6392520e",
      "metadata": {
        "id": "6392520e",
        "outputId": "bd1ee7f4-26df-4b2f-f06c-a8ac40e65ebe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "CPU times: total: 2min 22s\n",
            "Wall time: 3min 58s\n"
          ]
        }
      ],
      "source": [
        "### a memory-efficient way to dump and load trigram using itertools and generators\n",
        "\n",
        "%%time\n",
        "def chunks(data, SIZE):\n",
        "    it = iter(data)\n",
        "    for i in range(0, len(data), SIZE):\n",
        "        yield {k:data[k] for k in islice(it, SIZE)}\n",
        "\n",
        "def dump_trigram():\n",
        "    batch = 0\n",
        "    for item in chunks(trigram_model,int(len(trigram_model)/5)):\n",
        "        print(batch)\n",
        "        pickle.dump(item,open('trigram_model'+ str(batch),'wb'),protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        batch += 1\n",
        "\n",
        "dump_trigram()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ec4faf4",
      "metadata": {
        "id": "7ec4faf4"
      },
      "outputs": [],
      "source": [
        "def load_trigram():\n",
        "    trigram = {}\n",
        "    for i in range(6):\n",
        "        file = open('trigram_model'+str(i),'rb')\n",
        "        batch = pickle.load(file)\n",
        "        trigram = {**trigram, **batch} # merges dictionaries\n",
        "\n",
        "    return trigram\n",
        "\n",
        "trigram_model = load_trigram()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b2dd0e5",
      "metadata": {
        "id": "2b2dd0e5",
        "outputId": "91eb5d6f-5779-488d-8846-6654ba51ca5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(19646034, 69904872)"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(bigram_model),len(trigram_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4524750",
      "metadata": {
        "id": "b4524750"
      },
      "outputs": [],
      "source": [
        "### predict_topten_trigram(target):\n",
        "### input: first,second word of trigram, word previous and previous to previous to the missing word (str)\n",
        "### output: dict with top-ten trigrams as keys and probability as values\n",
        "### probability is calculated using Laplacian smoothing\n",
        "### sort only after accessing the required bigrams, then taking first ten\n",
        "\n",
        "def predict_topten_bigram(target):\n",
        "    topten = {}\n",
        "    for ngram in bigram_model.keys():\n",
        "        if ngram[0] == target and ngram[1] != 'endsntnc':\n",
        "\n",
        "            prob = (bigram_model[ngram]+1)/(unigram_model[target] + len(unigram_model))\n",
        "\n",
        "            topten[ngram] = prob\n",
        "#             print((ngram, model[ngram]))\n",
        "\n",
        "    topten = dict(sorted(topten.items(), key = lambda x : x[1], reverse=True))\n",
        "\n",
        "    return dict(islice(topten.items(),10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0101fc96",
      "metadata": {
        "id": "0101fc96"
      },
      "outputs": [],
      "source": [
        "### predict_topten_trigram(target1,target2):\n",
        "### input: first,second word of trigram, word previous and previous to previous to the missing word (str)\n",
        "### output: dict with top-ten trigrams as keys and probability as values\n",
        "### probability is calculated using Laplacian smoothing\n",
        "### sort only after accessing the required bigrams, then taking first ten\n",
        "\n",
        "def predict_topten_trigram(target1, target2):\n",
        "    topten = {}\n",
        "    for ngram in trigram_model.keys():\n",
        "        if ngram[0] == target1 and ngram[1] == target2 and ngram[2] != 'endsntnc':\n",
        "\n",
        "            prob = (trigram_model[ngram]+1)/(bigram_model[(target1,target2)] + len(bigram_model))\n",
        "\n",
        "            topten[ngram] = prob\n",
        "#             print((ngram, model[ngram]))\n",
        "\n",
        "    topten = dict(sorted(topten.items(), key = lambda x : x[1], reverse=True))\n",
        "\n",
        "    return dict(islice(topten.items(),10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0429d77",
      "metadata": {
        "id": "c0429d77"
      },
      "source": [
        "The ```bigram_model``` and ```trigram_model``` variables contain the bigram and trigram models respectively.\n",
        "\n",
        "We save them in the disk using ```pickle.dump(model,file)```."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b35d1c6e",
      "metadata": {
        "id": "b35d1c6e"
      },
      "source": [
        "## Predicting the missing text\n",
        "\n",
        "To predict the missing word, we extract the n words previous to it for a n-gram model, and extract the relevant n-grams from the dictionary of all n-grams, and then select the one with the highest probabillity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "124f0a5c",
      "metadata": {
        "id": "124f0a5c"
      },
      "outputs": [],
      "source": [
        "### pred_miss_word_bigram(sent):\n",
        "### input: sentence (str)\n",
        "### output: prints top-ten bigrams with probability and fills the blank with the bigram with highest probability\n",
        "\n",
        "\n",
        "def pred_miss_word_bigram(sent):\n",
        "\n",
        "    sent = sent.replace(\"___\", \"xxxmissingwordxxx\")\n",
        "    prep_sent = preprocessing([[sent]])\n",
        "    sent_pad = \"bgnsntnc \" + prep_sent + \" endsntnc\"\n",
        "\n",
        "    print(\"Preprocessed sentence: \",sent_pad)\n",
        "    words = sent_pad.split()\n",
        "\n",
        "    for i in range(1,len(words)):\n",
        "        if words[i] == \"xxxmissingwordxxx\":\n",
        "            target = words[i-1]\n",
        "\n",
        "            topten_bigram = predict_topten_bigram(target)\n",
        "\n",
        "            print(*topten_bigram.items(), sep='\\n')\n",
        "\n",
        "            words[i] = sorted(topten_bigram.items(), key = lambda x : x[1], reverse=True)[0][0][1]\n",
        "\n",
        "    print(\"-----------------------------------***-----------------------------------\")\n",
        "    print(\"Filled sentence: \", \" \".join(words))\n",
        "    print(\"-----------------------------------***-----------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a67cbb74",
      "metadata": {
        "id": "a67cbb74"
      },
      "outputs": [],
      "source": [
        "### pred_miss_word_bigram(sent):\n",
        "### input: sentence (str)\n",
        "### output: prints top-ten bigrams with probability and fills the blank with the bigram with highest probability\n",
        "\n",
        "def pred_miss_word_trigram(sent):\n",
        "\n",
        "    sent = sent.replace(\"___\", \"xxxmissingwordxxx\")\n",
        "    prep_sent = preprocessing([[sent]])\n",
        "    sent_pad = \"bgnsntnc bgnsntnc \" + prep_sent + \" endsntnc endsntnc\"\n",
        "\n",
        "    print(\"Preprocessed sentence: \",sent_pad)\n",
        "    words = sent_pad.split()\n",
        "\n",
        "    for i in range(2,len(words)):\n",
        "        if words[i] == \"xxxmissingwordxxx\":\n",
        "            target1 = words[i-2]\n",
        "            target2 = words[i-1]\n",
        "            topten_trigram = predict_topten_trigram(target1,target2)\n",
        "\n",
        "            print(*topten_trigram.items(), sep='\\n')\n",
        "\n",
        "            words[i] = sorted(topten_trigram.items(), key = lambda x : x[1], reverse=True)[0][0][2]\n",
        "\n",
        "    print(\"-----------------------------------***-----------------------------------\")\n",
        "    print(\"Filled sentence: \", \" \".join(words))\n",
        "    print(\"-----------------------------------***-----------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68dd6c50",
      "metadata": {
        "id": "68dd6c50"
      },
      "outputs": [],
      "source": [
        "test_sentences_pred = [\"all houses were ___ ventilated\",\n",
        "                  \"it aims to develop an integrated ___ to reach mmps exposed to malaria with prevention diagnosis and treatment ___ by involving non-health ___ stakeholders from provincial to community level\",\n",
        "                  \"this is because engineers do not work in ___ but rather as a team\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "563e886d",
      "metadata": {
        "id": "563e886d",
        "outputId": "3b862d42-1a55-4e36-e4bc-7fffaf99f645"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessed sentence:  bgnsntnc all houses were xxxmissingwordxxx ventilated endsntnc\n",
            "(('were', 'not'), 0.01245136281178955)\n",
            "(('were', 'used'), 0.012113702847909056)\n",
            "(('were', 'also'), 0.009276146001742083)\n",
            "(('were', 'performed'), 0.008972049842654501)\n",
            "(('were', 'found'), 0.007881832761670296)\n",
            "(('were', 'collected'), 0.007563987574113329)\n",
            "(('were', 'obtained'), 0.006384806191268397)\n",
            "(('were', 'observed'), 0.005918552372880122)\n",
            "(('were', 'identified'), 0.005815030276169456)\n",
            "(('were', 'detected'), 0.005102507094903073)\n",
            "-----------------------------------***-----------------------------------\n",
            "Filled sentence:  bgnsntnc all houses were not ventilated endsntnc\n",
            "-----------------------------------***-----------------------------------\n",
            "None\n",
            "Preprocessed sentence:  bgnsntnc it aims to develop an integrated xxxmissingwordxxx to reach mmps exposed to malaria with prevention diagnosis and treatment xxxmissingwordxxx by involving nonhealth xxxmissingwordxxx stakeholders from provincial to community level endsntnc\n",
            "(('integrated', 'into'), 0.0011654415444324589)\n",
            "(('integrated', 'with'), 0.0006108930232902965)\n",
            "(('integrated', 'in'), 0.0003358428878040101)\n",
            "(('integrated', 'and'), 0.0002794983856558759)\n",
            "(('integrated', 'dna'), 0.00020461950780111874)\n",
            "(('integrated', 'approach'), 0.0002038781327728538)\n",
            "(('integrated', 'to'), 0.00012751650486156674)\n",
            "(('integrated', 'the'), 0.0001193613795506526)\n",
            "(('integrated', 'moving'), 0.00010230975390055937)\n",
            "(('integrated', 'care'), 0.00010156837887229445)\n",
            "(('treatment', 'of'), 0.022999417180266997)\n",
            "(('treatment', 'with'), 0.009048456642210965)\n",
            "(('treatment', 'and'), 0.006418228555806793)\n",
            "(('treatment', 'for'), 0.0051627732254319195)\n",
            "(('treatment', 'in'), 0.003170434453103335)\n",
            "(('treatment', 'is'), 0.003046527895692969)\n",
            "(('treatment', 'was'), 0.0025115662510005947)\n",
            "(('treatment', 'group'), 0.0013347817507275413)\n",
            "(('treatment', 'options'), 0.0013315037994732987)\n",
            "(('treatment', 'or'), 0.001260700052381661)\n",
            "(('nonhealth', 'care'), 3.215554309216676e-05)\n",
            "(('nonhealth', 'sectors'), 2.0938493176294635e-05)\n",
            "(('nonhealth', 'workers'), 2.0938493176294635e-05)\n",
            "(('nonhealth', 'consequences'), 1.1964853243596934e-05)\n",
            "(('nonhealth', 'sector'), 1.0469246588147317e-05)\n",
            "(('nonhealth', 'numeracy'), 7.478033277248084e-06)\n",
            "(('nonhealth', 'related'), 5.982426621798467e-06)\n",
            "(('nonhealth', 'gdp'), 5.982426621798467e-06)\n",
            "(('nonhealth', 'professionals'), 5.982426621798467e-06)\n",
            "(('nonhealth', 'actors'), 4.48681996634885e-06)\n",
            "-----------------------------------***-----------------------------------\n",
            "Filled sentence:  bgnsntnc it aims to develop an integrated into to reach mmps exposed to malaria with prevention diagnosis and treatment of by involving nonhealth care stakeholders from provincial to community level endsntnc\n",
            "-----------------------------------***-----------------------------------\n",
            "None\n",
            "Preprocessed sentence:  bgnsntnc this is because engineers do not work in xxxmissingwordxxx but rather as a team endsntnc\n",
            "(('in', 'the'), 0.1762119260517278)\n",
            "(('in', 'a'), 0.03319178131981444)\n",
            "(('in', 'this'), 0.023735894380854747)\n",
            "(('in', 'addition'), 0.013375827151531688)\n",
            "(('in', 'patients'), 0.010848005932272448)\n",
            "(('in', 'our'), 0.008278361395617547)\n",
            "(('in', 'order'), 0.0077739921036912895)\n",
            "(('in', 'which'), 0.007505625817069125)\n",
            "(('in', 'vitro'), 0.006336730482078459)\n",
            "(('in', 'an'), 0.006090935509606335)\n",
            "-----------------------------------***-----------------------------------\n",
            "Filled sentence:  bgnsntnc this is because engineers do not work in the but rather as a team endsntnc\n",
            "-----------------------------------***-----------------------------------\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "for sentence in test_sentences_pred:\n",
        "    print(pred_miss_word_bigram(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "064bc84c",
      "metadata": {
        "id": "064bc84c",
        "outputId": "ebc6067f-baf6-4768-f4b2-fe64d6c636e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessed sentence:  bgnsntnc bgnsntnc all houses were xxxmissingwordxxx ventilated endsntnc endsntnc\n",
            "(('houses', 'were', 'made'), 1.5270227286644003e-07)\n",
            "(('houses', 'were', 'built'), 1.5270227286644003e-07)\n",
            "(('houses', 'were', 'investigated'), 1.0180151524429336e-07)\n",
            "(('houses', 'were', 'malaria'), 1.0180151524429336e-07)\n",
            "(('houses', 'were', 'contacted'), 1.0180151524429336e-07)\n",
            "(('houses', 'were', 'tested'), 1.0180151524429336e-07)\n",
            "(('houses', 'were', 'then'), 1.0180151524429336e-07)\n",
            "(('houses', 'were', 'no'), 1.0180151524429336e-07)\n",
            "(('houses', 'were', 'not'), 1.0180151524429336e-07)\n",
            "(('houses', 'were', 'temperature'), 1.0180151524429336e-07)\n",
            "-----------------------------------***-----------------------------------\n",
            "Filled sentence:  bgnsntnc bgnsntnc all houses were made ventilated endsntnc endsntnc\n",
            "-----------------------------------***-----------------------------------\n",
            "None\n",
            "Preprocessed sentence:  bgnsntnc bgnsntnc it aims to develop an integrated xxxmissingwordxxx to reach mmps exposed to malaria with prevention diagnosis and treatment xxxmissingwordxxx by involving nonhealth xxxmissingwordxxx stakeholders from provincial to community level endsntnc endsntnc\n",
            "(('an', 'integrated', 'approach'), 8.041689748352116e-06)\n",
            "(('an', 'integrated', 'and'), 3.35918685690658e-06)\n",
            "(('an', 'integrated', 'system'), 2.443044986841149e-06)\n",
            "(('an', 'integrated', 'model'), 1.8831805106900524e-06)\n",
            "(('an', 'integrated', 'analysis'), 1.3742128050981462e-06)\n",
            "(('an', 'integrated', 'framework'), 1.3233160345389557e-06)\n",
            "(('an', 'integrated', 'platform'), 1.3233160345389557e-06)\n",
            "(('an', 'integrated', 'part'), 1.0688321817430027e-06)\n",
            "(('an', 'integrated', 'health'), 1.0179354111838121e-06)\n",
            "(('an', 'integrated', 'view'), 9.670386406246214e-07)\n",
            "(('and', 'treatment', 'of'), 0.00018145815130923813)\n",
            "(('and', 'treatment', 'with'), 2.0908130134033324e-05)\n",
            "(('and', 'treatment', 'for'), 1.236174117413649e-05)\n",
            "(('and', 'treatment', 'and'), 1.0937343014153685e-05)\n",
            "(('and', 'treatment', 'strategies'), 1.0377758022731868e-05)\n",
            "(('and', 'treatment', 'in'), 9.563816217027409e-06)\n",
            "(('and', 'treatment', 'is'), 9.411202128457821e-06)\n",
            "(('and', 'treatment', 'options'), 8.444646234183776e-06)\n",
            "(('and', 'treatment', 'are'), 7.986803968475016e-06)\n",
            "(('and', 'treatment', 'protocol'), 6.104563542783452e-06)\n",
            "(('involving', 'nonhealth', 'sector'), 1.0180171215209583e-07)\n",
            "-----------------------------------***-----------------------------------\n",
            "Filled sentence:  bgnsntnc bgnsntnc it aims to develop an integrated approach to reach mmps exposed to malaria with prevention diagnosis and treatment of by involving nonhealth sector stakeholders from provincial to community level endsntnc endsntnc\n",
            "-----------------------------------***-----------------------------------\n",
            "None\n",
            "Preprocessed sentence:  bgnsntnc bgnsntnc this is because engineers do not work in xxxmissingwordxxx but rather as a team endsntnc endsntnc\n",
            "(('work', 'in', 'the'), 4.356123032181215e-05)\n",
            "(('work', 'in', 'this'), 1.5775679205329167e-05)\n",
            "(('work', 'in', 'a'), 1.5063229176701397e-05)\n",
            "(('work', 'in', 'progress'), 4.936260912635255e-06)\n",
            "(('work', 'in', 'ensuring'), 4.529146610562245e-06)\n",
            "(('work', 'in', 'our'), 3.969364445211855e-06)\n",
            "(('work', 'in', 'an'), 3.7658072941753494e-06)\n",
            "(('work', 'in', 'concert'), 3.3586929921023386e-06)\n",
            "(('work', 'in', 'other'), 2.340907236919812e-06)\n",
            "(('work', 'in', 'order'), 1.9846822226059275e-06)\n",
            "-----------------------------------***-----------------------------------\n",
            "Filled sentence:  bgnsntnc bgnsntnc this is because engineers do not work in the but rather as a team endsntnc endsntnc\n",
            "-----------------------------------***-----------------------------------\n",
            "None\n",
            "CPU times: total: 3min 59s\n",
            "Wall time: 13min 2s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "for sentence in test_sentences_pred:\n",
        "    print(pred_miss_word_trigram(sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "839c6d04",
      "metadata": {
        "id": "839c6d04"
      },
      "source": [
        "## Perplexity\n",
        "\n",
        "Calculated using the formula $$\\text{Perplexity}(W) =  \\left( \\frac{1}{\\prod_{i=1}^N P(w_i|w_1,w_2,\\ldots,w_{i-1})}  \\right)^{1/N}$$\n",
        "where $W$ is the sentence and $w_i$ are the words. We calculate the probabilities using chain rule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a2549ab",
      "metadata": {
        "id": "3a2549ab"
      },
      "outputs": [],
      "source": [
        "### perplexity_score_bigram(sent):\n",
        "### input: sentence (str)\n",
        "### output: perplexity score (float)\n",
        "### tags are added because it is a full sentence, so first bigram is (\"tag\",\"firstword\"), and so on.\n",
        "\n",
        "\n",
        "def perplexity_score_bigram(sent):\n",
        "    sent_pad = \"bgnsntnc \" + sent + \" endsntnc\"\n",
        "    words = sent_pad.split()\n",
        "    prob = 1\n",
        "    for i in range(len(words)-1):\n",
        "        bigram = (words[i],words[i+1])\n",
        "        prob *= (bigram_model[bigram]+1)/(unigram_model[bigram[0]] + len(unigram_model))\n",
        "\n",
        "    return 1/prob**(1/len(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5716773",
      "metadata": {
        "id": "e5716773",
        "outputId": "79c53c66-c0b0-4dee-d162-ced35db642cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "540.3838731126516"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "perplexity_score_bigram(\"it appears that the overall code stroke volume has decreased since the covid pandemic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6952ec54",
      "metadata": {
        "scrolled": true,
        "id": "6952ec54",
        "outputId": "92408678-1080-4a0e-d36f-2e0a5f12d4cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3215.0028205315984"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "perplexity_score_bigram(\"half a century ago hypertension was not treatable\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "052a2cd8",
      "metadata": {
        "id": "052a2cd8",
        "outputId": "31c83a6e-c1de-4001-b0c1-3fe4ec4dec59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "31731.14663008426"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "perplexity_score_bigram(\"sarahs tv is broadcasting an advert for private healthcare\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49923a70",
      "metadata": {
        "id": "49923a70"
      },
      "outputs": [],
      "source": [
        "### perplexity_score_bigram(sent):\n",
        "### input: sentence (str)\n",
        "### output: perplexity score (float)\n",
        "### tags are added because it is a full sentence, so first trigram is (\"tag\",\"tag\",\"firstword\"), and so on.\n",
        "\n",
        "def perplexity_score_trigram(sent):\n",
        "    sent_pad = \"bgnsntnc bgnsntnc \" + sent + \" endsntnc endsntnc\"\n",
        "    words = sent_pad.split()\n",
        "    prob = 1\n",
        "    for i in range(len(words)-2):\n",
        "        trigram = (words[i],words[i+1],words[i+2])\n",
        "#         print((bigram_model_ord[bigram]+1)/(unigram_model[bigram[0]] + len(unigram_model)))\n",
        "        prob *= (trigram_model[trigram]+1)/(bigram_model[(trigram[0],trigram[1])] + len(bigram_model))\n",
        "\n",
        "    return 1/prob**(1/len(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0679378",
      "metadata": {
        "id": "f0679378",
        "outputId": "db663d35-be84-47c4-b6a3-6cae9846e2c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "25399.76565179547"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "perplexity_score_trigram(\"it appears that the overall code stroke volume has decreased since the covid pandemic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c2d31f3",
      "metadata": {
        "id": "6c2d31f3",
        "outputId": "3f5563f0-89e1-4abd-c513-e9847066fff1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "105619.44405951035"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "perplexity_score_trigram(\"half a century ago hypertension was not treatable\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ab6e886",
      "metadata": {
        "id": "5ab6e886",
        "outputId": "15c46033-0199-4697-a2ca-fc4f8cda4817"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "456819.137728019"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "perplexity_score_trigram(\"sarahs tv is broadcasting an advert for private healthcare\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b9a4ee6",
      "metadata": {
        "id": "6b9a4ee6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}